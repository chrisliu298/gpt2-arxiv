# CSE142 Project: Conditional Text Generation with GPT-2

Our initial thoughts for this project is to re-implement GPT-2 from scratch, so it is challenging than all other project. To complete this project successfully, an estimated list of works includes but not limits to:

1. Fully understand the GPT-2 paper

2. Fully understand the GPT-2 source code

## Project Topics

[Project Guidelines](CSE_142_Project.pdf) | [Project sign-up sheet](https://docs.google.com/spreadsheets/d/130Ehe4mHSwk2j01xbEVWBWpgTXs0mNkg7XVewQ-2Ejc/edit?usp=sharing)


## Possible Project Ideas

1. Fine-tune GPT-2 on the Amazon Reviews dataset.

2. Fine-tune GPT-2 on the IMDb dataset.

3. Fine-tune GPT-2 on books.


## Useful Resources 

Resources listed below are only tentative.

### Papers

- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

- [Transformers: State-of-the-art Natural Language Processing](https://arxiv.org/pdf/1910.03771v4.pdf)


### Source Code

- [OpenAI GPT-2](https://github.com/openai/gpt-2)

- [Huggingface Transformers](https://github.com/huggingface/transformers)


### Docs

- [Huggingface Transformers Documentation](https://huggingface.co/transformers/)

- [PyTorch seq2seq Modeling Tutorial (some components of the GPT-2 model)](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)


### Blogs

- [Better Language Models and Their Implications](https://openai.com/blog/better-language-models/) (OpenAI)

- [GPT-2: 6-Month Follow-Up](https://openai.com/blog/gpt-2-6-month-follow-up/) (OpenAI)

- [GPR-2: 1.5B Release](https://www.openai.com/blog/gpt-2-1-5b-release/) (OpenAI)

- [The Illustrated GPT-2 (Visualizing Transformer Language Models)](http://jalammar.github.io/illustrated-gpt2/)

- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)

- [The Annotated Transformers](https://nlp.seas.harvard.edu/2018/04/03/attention.html)

- [The Annotated GPT-2](https://amaarora.github.io/2020/02/18/annotatedGPT2.html)

- [How to train a new language model from scratch using Transformers and Tokenizers](https://huggingface.co/blog/how-to-train)


### Others

- [fastai forum re-implementing GPT-2 from scratch](https://forums.fast.ai/t/need-help-with-implementing-gpt-2-from-scratch/62189/3)