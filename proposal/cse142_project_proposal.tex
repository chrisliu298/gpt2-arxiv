\documentclass[10pt]{article}
\usepackage{arxiv}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{float}
\usepackage{multicol}
\usepackage{biblatex}
\hypersetup{colorlinks=true, urlcolor=blue}
\setlength{\tabcolsep}{5pt}
\graphicspath{{./images/}}
\renewcommand{\baselinestretch}{1.1}
\newcommand{\heart}{\ensuremath\heartsuit}
\DeclareMathOperator*{\argmax}{arg\,max}
\addbibresource{references.bib}


\title{Conditional Synthetic Text Generation with Fine-tuned GPT-2}
\author{
 Chris Liu \\
    \texttt{yliu298@ucsc.edu} \\
 \And
 Yiyun Zheng \\
    \texttt{yzheng63@ucsc.edu}
}

 

\begin{document}
\maketitle
\section{Project Idea}

Language modeling is the task of predicting the next word or character, given all previous words. GPT-2, the successor of GPT \cite{radford2018improving} trained by OpenAI \cite{radford2019language} using 40GB Internet raw text data, is the state-of-the-art language model to perform such task. However, its performance can largely depend on its familiarity with the target topic. Our task in this project is to fine-tune the GPT-2 model on our datasets, which consist of submissions of different topics scraped from multiple subreddit topics, to perform conditional synthetic text samples generation. Our goal is to explore GPT-2's strength and flaw after fine-tuning it on a domain-specific topic.

\section{Dataset Description}

At this point, we implemented a scraper to scrape the submissions under a particular subreddit posted after a specific time. To ensure the text quality, we only kept the submissions with at least three comments. The dataset of a subreddit is a \texttt{.txt} file, and each submission in the file start with a start token \texttt{<|startoftext|>} and ends with an end token \texttt{<|endoftext|>}. After all the data are collected, it pre-processes and splits it into training, validation, and testing set. Note that in the split sets, the start token \texttt{<|startoftext|>} will serve as a delimiter and be omitted because the model does not need to learn how to start a sentence, but the end token \texttt{<|endoftext|>} will be kept to separate the submissions and to get the model of sense of how to stop a story.

Using pre-trained model allows us to spend extra effort on experimenting multiple datasets. If we are not restricted by the time and the computing power, we will try other ideas to make this project more interesting. One example can be generating digital versions of people by fine-tuning the GPT-2 on their tweets.

\section{Input-Output Behavior}

The input of the model is a string. It can be a single word or a paragraph. The output of the model is a string of user defined length, which also includes the input string at the front. Since the probability of the output sentences or paragraphs will condition on the give input words or sentences, we expect the output to correlate with the prompt, but the correlation will depend on the model setting.

\section{Evaluation Metrics}

One common method to evaluate language models is calculating the perplexity on the test set.

\clearpage

\printbibliography 

\end{document}
