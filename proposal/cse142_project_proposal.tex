\documentclass[10pt]{article}
\usepackage{arxiv}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{float}
\usepackage{multicol}
\usepackage{biblatex}
\hypersetup{colorlinks=true, urlcolor=red}
\setlength{\tabcolsep}{5pt}
\graphicspath{{./images/}}
\renewcommand{\baselinestretch}{1.1}
\newcommand{\heart}{\ensuremath\heartsuit}
\DeclareMathOperator*{\argmax}{arg\,max}
\addbibresource{references.bib}

\title{Conditional Text Generation with Fine-tuned GPT-2}
\author{
 Chris Liu \\
    \texttt{yliu298@ucsc.edu} \\
 \And
 Yiyun Zheng \\
    \texttt{yzheng63@ucsc.edu}
}
\date{}
 
\begin{document}
\maketitle
\section{Project Idea}

Language modeling is the task of predicting the next word or character given all previous words. GPT-2, the successor of GPT \cite{}trained by OpenAI \cite{radford2019language} using 40GB Internet text data, is the state-of-the-art language model to perform such task. It is a transformer-based model with four variations of model size (number of parameters): \texttt{124M}, \texttt{355M}, \texttt{774M}, and \texttt{1558M}

Our mission in this project is to 

\section{Dataset Description}

\section{Input-Output Behavior}

\section{Evaluation Metrics}

\clearpage

\printbibliography 

\end{document}
