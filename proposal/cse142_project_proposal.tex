\documentclass[10pt]{article}
\usepackage{arxiv}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{float}
\usepackage{multicol}
\usepackage{biblatex}
\hypersetup{colorlinks=true, urlcolor=red}
\setlength{\tabcolsep}{5pt}
\graphicspath{{./images/}}
\renewcommand{\baselinestretch}{1.1}
\newcommand{\heart}{\ensuremath\heartsuit}
\DeclareMathOperator*{\argmax}{arg\,max}
\addbibresource{references.bib}

\title{Conditional Text Generation with Fine-tuned GPT-2}
\author{
 Chris Liu \\
    \texttt{yliu298@ucsc.edu} \\
 \And
 Yiyun Zheng \\
    \texttt{yzheng63@ucsc.edu}
}
 

\begin{document}
\maketitle
\section{Project Idea}

GPT-2 \cite{radford2019language} is a language model trained by OpenAI to predict the next word using 40GB Internet text data. 

\section{Dataset Description}

\section{Input-Output Behavior}

\section{Evaluation Metrics}

\clearpage

\printbibliography 

\end{document}
