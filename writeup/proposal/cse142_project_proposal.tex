\documentclass[10pt]{article}
\usepackage{arxiv}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{float}
\usepackage{multicol}
\usepackage{biblatex}
\hypersetup{colorlinks=true, urlcolor=blue}
\setlength{\tabcolsep}{5pt}
\graphicspath{{./images/}}
\renewcommand{\baselinestretch}{1.1}
\newcommand{\heart}{\ensuremath\heartsuit}
\DeclareMathOperator*{\argmax}{arg\,max}
\addbibresource{references.bib}


\title{Conditional Synthetic Text Generation with Fine-tuned GPT-2}
\author{
 Chris Liu \\
    \texttt{yliu298@ucsc.edu} \\
 \And
 Yiyun Zheng \\
    \texttt{yzheng63@ucsc.edu}
}

 

\begin{document}
\maketitle
\section{Project Idea}

Language modeling is the task of predicting the next word or character, given all previous words. This approach can be simply expressed as the joint probabilities over a sequence of $n$ tokens $(s_1, s_2, ... , s_n)$ as the product of conditional probabilities \cite{radford2019language}:
\begin{equation}
   p(x)=\prod_{i=1}^{n} p\left(s_{n} | s_{1}, \dots, s_{n-1}\right)
\end{equation}

GPT-2, the successor of GPT \cite{radford2018improving} trained by OpenAI \cite{radford2019language} using 40GB Internet raw text data, is the state-of-the-art language model to perform such task. However, its performance can largely depend on its familiarity with the target topic. Our task in this project is to fine-tune the GPT-2 model on our datasets, which consist of submissions of different topics scraped from multiple subreddit topics, to perform conditional synthetic text samples generation. Our goal is to explore GPT-2's strengths and flaws after fine-tuning it on a domain-specific topic.

\section{Dataset Description}

At this point, we implemented a scraper to scrape the submissions under a particular subreddit posted after a specific time. To ensure the text quality, we only kept the submissions with at least three comments. The dataset of a subreddit is a \texttt{.txt} file, and each submission in the file start with a start token \texttt{<|startoftext|>} and ends with an end token \texttt{<|endoftext|>}. After all the data are collected, it pre-processes and splits it into training, validation, and testing set. Note that in the split sets, the start token \texttt{<|startoftext|>} will serve as a delimiter and be omitted because the model does not need to learn how to start a sentence, but the end token \texttt{<|endoftext|>} will be kept to separate the submissions and to get the model of sense of how to stop a story.

Using the pre-trained model allows us to spend extra effort on experimenting with multiple datasets. If we are not restricted by the time and the computing power, we will try other ideas to make this project more enjoyable. One example can be generating digital versions of people by fine-tuning the GPT-2 on their tweets.

\section{Input-Output Behavior}

The input of the model is a string. It can be a single word or a paragraph. The output of the model is a string of a user-defined length with the input string at the front. Since the probability of the output sentences or paragraphs will condition on the give input words or sentences, we expect the output to correlate with the prompt, but the correlation will depend on the model setting.

\section{Evaluation Metrics}

\paragraph{Downstream Tasks} Evaluating language models on downstream tasks is the best way to test its performance. One way of doing it can be doing the sentiment analysis task using the learned word embedding from GPT-2 on a simple RNN model. We may use project one as an experimenting procedure for this evaluation.

\paragraph{Perplexity} Perplexity score is one way of measuring the performance of a language model. Since a language model is essentially a probability distribution over the sentences, a high-quality one assigns a high probability to the unseen sentences. Therefore, the perplexity of a language model $p$ on a dataset $\bm{\overline{x}_{1:m}}$ with $m$ tokens \cite{smith_probabilistic_nodate} is:

\begin{equation}
   \text{perplexity}(p; \bm{\overline{x}_{1:m}}) = 2^{\displaystyle \left(\frac{1}{M} \sum_{i=1}^{m}-\log _{2} p\left(\overline{\boldsymbol{x}}_{i}\right)\right)}
\end{equation}

The perplexity is essentially exponentiation of the cross-entropy loss. Note that the calculation of perplexity for GPT-2 may differ from the equation above, and we will address this in the progress report more.

\paragraph{Human Evaluation} The human evaluation of the fine-tuned model will be 1) survey people to assign a credibility score of the output text and 2) letting humans distinguish between the generated text and the human-written text.

\clearpage

\printbibliography 

\end{document}
